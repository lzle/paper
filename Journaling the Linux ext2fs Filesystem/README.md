# Journaling the Linux ext2fs Filesystem

> Stephen C. Tweedie

> Linux Expo 1998（LinuxExposition, Durham, North Carolina, USA），1998

## 目录
* [Abstract](#abstract)
* [Introduction](#introduction)
  * [What's in a filesystem?](#whats-in-a-filesystem)
  * [Filesystem Reliability](#filesystem-reliability)
  * [Existing implementations](#existing-implementations)
* [Designing a new filesystem for Linux](#designing-a-new-filesystem-for-linux)
  * [Anatomy of a transaction](#anatomy-of-a-transaction)
  * [Merging transactions](#merging-transactions)
  * [On-disk representation](#on-disk-representation)
  * [Format of the filesystem journal](#format-of-the-filesystem-journal)
  * [Committing and checkpointing the journal](#committing-and-checkpointing-the-journal)
  * [Collisions between transactions](#collisions-between-transactions)
* [Project status and future work](#project-status-and-future-work)
* [Conclusions](#conclusions)

## Abstract

本文描述了一项正在进行的工作，旨在为 Linux ext2fs 文件系统设计和实现一个事务性元数据日志。我们回顾了崩溃后文件系统的恢复问题，并介绍了一种设计，该设计旨在通过向文件系统添加事务性日志来提高 ext2fs 崩溃恢复的速度和可靠性。。

## Introduction

文件系统是任何现代操作系统的核心部分，人们期望它既快速又极其可靠。然而，问题仍然会出现，由于硬件、软件或电源故障，机器可能会意外宕机。

系统意外重启后，可能需要一些时间才能将其文件系统恢复到一致状态。随着磁盘容量的增大，这段时间可能会成为一个严重的问题，导致系统在磁盘扫描、检查和修复期间离线一小时甚至更长时间。尽管磁盘驱动器每年都在变得更快，但与它们容量的巨大增长相比，这种速度的提升显得微不足道。不幸的是，当使用传统的文件系统检查技术时，磁盘容量每增加一倍，恢复时间也会增加一倍。

在系统可用性至关重要的情况下，这可能是一段不容浪费的时间，因此需要一种机制，以避免每次机器重启时都需要进行昂贵的恢复阶段。

### What’s in a filesystem?

我们对任何文件系统有哪些功能要求？存在一些明显的要求，这些要求由文件系统所服务的操作系统决定。文件系统呈现给应用程序的方式就是其中之一！操作系统通常要求文件名遵循特定的规范，并且文件具有某些属性，这些属性会以特定方式被解读。

然而，文件系统有许多内部方面的限制并没有那么严格，文件系统的实现者可以在一定程度上自由设计。数据在磁盘上的布局（或者，如果文件系统不是本地的，其网络协议的细节）、内部缓存的详细信息以及用于调度磁盘输入输出的算法——这些都是可以改变的，且不一定会违反文件系统应用程序接口的规范。

我们选择一种设计而非另一种设计，可能有多种原因。与较旧的文件系统的兼容性可能是一个问题：例如，Linux 提供了一种 UMSDOS 文件系统，它在标准 MSDOS 的磁盘文件结构之上实现了 POSIX 文件系统的语义。

在尝试解决 Linux 系统上文件系统恢复时间过长的问题时，我们牢记了多个目标：

* 使用新文件系统不应导致性能严重下降；

* 不得破坏与现有应用程序的兼容性；

* 文件系统的可靠性绝不能以任何方式受到损害。

### Filesystem Reliability

当我们谈论文件系统的可靠性时，有许多问题至关重要。就这个特定项目而言，我们主要关注的是从崩溃的文件系统中恢复内容的可靠性，并且我们可以确定这方面的几个要点：

**保护性**：崩溃前在磁盘上保持稳定的数据绝不能被损坏。显然，崩溃时正在写入的文件无法保证完全完好，但任何已安全存储在磁盘上的文件都不得被恢复系统触碰。

**可预测性**：我们必须从中进行恢复的那些故障模式应当是**可预测的**，唯有如此，系统才能实现**可靠的恢复**。

**原子性**：许多文件系统操作需要大量独立的输入/输出操作才能完成。一个很好的例子是将文件从一个目录重命名到另一个目录。如果此类文件系统操作在恢复完成后要么在磁盘上完全完成，要么完全撤销，那么恢复就是原子性的。（对于重命名示例，崩溃后恢复应仅保留提交到磁盘的旧文件名或新文件名，而不能两者都保留。）

### Existing implementations

Linux 的 ext2fs 文件系统提供了保留恢复功能，但它是非原子性且不可预测的。实际上，可预测性是一个比初看起来复杂得多的特性。为了能够在崩溃后可预测地进行清理，恢复阶段必须能够弄清楚文件系统当时正在尝试做什么，如果遇到代表磁盘上未完成操作的不一致情况。通常，这要求当单个更新操作更改磁盘上的多个块时，文件系统必须以可预测的顺序将其写入磁盘。

> 注：例如，当新增一个文件时会涉及多个磁盘块的写入，可以先写入数据块，再写入元数据、目录块、位图块等。

实现磁盘写入之间的这种顺序有多种方法。最简单的方法就是在将下一批写入提交给设备驱动程序之前，等待第一批写入完成！这就是“同步元数据更新”方法。BSD 快速文件系统[1]就采用了这种方法，该文件系统出现在 4.2BSD 中，并启发了许多后续的Unix 文件系统，包括 ext2fs。

然而，同步元数据更新的一大缺点是其性能。如果文件系统操作要求我们等待磁盘输入输出完成，那么我们就无法将多个文件系统更新批量处理为一次磁盘写入。例如，如果我们在磁盘上的同一个目录块中创建十二个目录项，那么同步更新就要求我们将该块分别写回磁盘十二次。

有一些方法可以解决这个性能问题。一种既能保持磁盘写入顺序又不必实际等待输入/输出完成的方法是，在内存中维护磁盘缓冲区之间的顺序，并确保当我们最终要回写数据时，只有在所有前驱块都安全写入磁盘后，我们才会写入某个块！这就是“延迟有序写入”技术。

> 注：延迟有序写入是后台线程或I/O调度器负责最终提交，实际写入磁盘时检查依赖关系，CPU/应用线程无需等待。

延迟有序写入的一个复杂情况是，很容易出现缓存缓冲区之间存在循环依赖的局面。例如，如果我们尝试在两个目录之间重命名一个文件，同时将另一个文件从第二个目录重命名到第一个目录，那么最终会导致两个目录块相互依赖：只有当其中一个目录块写入磁盘后，另一个才能被写入。

Ganger 的“软更新”机制[2]巧妙地规避了这一问题：当我们首次尝试将缓冲区写入磁盘时，如果其中某些更新仍存在未解决的依赖关系，该机制会有选择地回滚缓冲区内的特定更新。一旦这些更新自身的所有依赖关系都得到满足，缺失的更新将在之后得到恢复。这使得当存在循环依赖时，我们可以按任意选择的顺序写出缓冲区。软更新机制已被 FreeBSD 采用，并将在其下一个主要内核版本中提供。

然而，所有这些方法都存在一个共同的问题。尽管它们确保了在文件系统操作的整个过程中，磁盘的状态始终处于可预测的状态，但恢复过程仍然需要扫描整个磁盘，以查找并修复任何未完成的操作。恢复变得更加可靠，但速度不一定更快。

> 注：同步元数据更新解决了顺序不一致导致崩溃恢复不可预测的问题，保障了可预测性，但当系统在操作中途崩溃时，仍然需要扫描整个磁盘找到不完整的操作。

然而，在不牺牲可靠性和可预测性的前提下，让文件系统恢复变得快速是有可能的。这通常由那些保证文件系统更新能原子性完成的文件系统来实现（在这类系统中，单次文件系统更新通常被称为事务）。原子更新的底层核心原理，是文件系统能够将一整批新数据一次性写入磁盘，但这些更新要等到在磁盘上完成最终的提交更新后才会生效。如果提交操作涉及向磁盘写入单个块，那么崩溃只会导致两种情况：要么提交记录已写入磁盘，在这种情况下，可以假定所有已提交的文件系统操作在磁盘上都是完整且一致的；要么提交记录缺失，此时我们必须忽略因崩溃时仍未完成的部分未提交更新而发生的任何其他写入。这自然要求文件系统更新将已更新数据的旧内容和新内容都保留在磁盘的某个位置，直到提交完成。

> 注：Copy-on-Write 不覆盖原始块，而是写入新副本。

有多种方法可以实现这一点。在某些情况下，文件系统会将更新数据的新副本存储在与旧副本不同的位置，一旦更新提交到磁盘，最终会重新利用旧空间。Network Appliance 的 WAFL 文件系统[6]就是这样工作的，它维护着一个文件系统数据树，只需将树节点复制到新位置，然后更新树根部的单个磁盘块，就能以原子方式更新整个树。

日志结构文件系统通过将所有文件系统数据（包括文件内容和元数据）以连续流（即"日志"）的形式写入磁盘来实现相同的目的。在这种方案下查找某一数据的位置可能比在传统文件系统中更复杂，但日志有一个很大的优势，即相对容易在日志中放置标记，以表明截至某一点的所有数据都已提交并在磁盘上保持一致。写入这种文件系统的速度也特别快，因为日志的特性使得大多数写入操作都以连续流的形式进行，无需磁盘寻道。基于这种设计的文件系统有很多，包括 Sprite LFS[3] 和 Berkeley LFS[4]。Linux 上也有一个 LFS 的原型实现[5]。

最后，有一种原子更新文件系统，在这类系统中，不完整更新的旧版本和新版本会通过将新版本写入磁盘上的单独位置来保存，直到更新被提交。提交后，文件系统可以自由地将已更新磁盘块的新版本写回它们在磁盘上的原始位置。

这就是“日志型”（有时也称为“日志增强型”）文件系统的工作方式。当磁盘上的元数据被更新时，这些更新会被记录在磁盘上一个专门预留的日志区域中。完成的文件系统事务会在日志中添加一条提交记录，只有当提交记录安全地写入磁盘后，文件系统才会将元数据写回其原始位置。事务是原子性的，因为在系统崩溃后，我们总能根据日志中是否包含该事务的提交记录，来决定是撤销事务（丢弃日志中的新数据）还是重做事务（将日志中的副本复制回原始副本）。许多现代文件系统都采用了这种设计的变体。


## Designing a new filesystem for Linux

这种针对 Linux 的新文件系统设计的主要动机是消除崩溃后极其漫长的文件系统恢复时间。出于这个原因，我们选择了文件系统日志记录方案作为这项工作的基础。日志记录能够实现快速的文件系统恢复，因为我们始终知道，所有可能在磁盘上存在不一致的数据都必须同时记录在日志中。因此，通过扫描日志并将所有已提交的数据复制回文件系统主区域，就可以完成文件系统的恢复。这一过程很快，因为日志通常比整个文件系统小得多。它只需要大到足以记录几秒钟内的未提交更新即可。

选择日志记录功能还有另一个重要优势。日志文件系统与传统文件系统的不同之处在于，它将临时数据存储在一个新的位置，与磁盘上的永久数据和元数据相互独立。正因为如此，这类文件系统并不要求永久数据必须以特定方式存储。具体来说，ext2fs 文件系统的磁盘结构完全可以在新文件系统中使用，而现有的 ext2fs 代码也可以作为日志版本的基础。

> 注：日志存储在磁盘上的单独区域，不会影响原有文件系统的数据和元数据，不用单独设计文件系统，直接在原有文件系统上添加日志功能。

因此，我们并非为 Linux 设计新的文件系统。相反，我们正在为现有的 ext2fs 添加一项新功能——事务性文件系统日志！

### Anatomy of a transaction

在考虑日志文件系统时，一个核心概念是事务，它对应于文件系统的一次单独更新。应用程序发出的任何一次文件系统请求都会产生恰好一个事务，该事务包含该请求所导致的所有已更改的元数据。例如，对文件的写入操作会导致磁盘上该文件的 inode 中的修改时间戳更新，如果此次写入操作使文件扩展，还可能会更新长度信息和块映射信息。如果为该文件分配了新块，那么配额信息、可用磁盘空间以及已用块位图都必须更新，而所有这些都必须记录在事务中。

事务中还有另一个我们必须注意的隐藏操作。事务还涉及读取文件系统的现有内容，这会在事务之间施加一种顺序。修改磁盘上某个块的事务，不能在读取了该新数据并根据所读内容更新磁盘的事务之后提交。即使这两个事务从未尝试写回相同的块，这种依赖关系也依然存在！想象一下，一个事务从目录中的一个块删除某个文件名，而另一个事务将相同的文件名插入到另一个块中。这两个操作在它们所写入的块上可能没有重叠，但第二个操作只有在第一个操作成功之后才有效（违反这一点会导致目录项重复）。

> 注：顺序约束，第二个事务依赖于第一个事务的写入结果，否则会导致目录项重复。

最后，有一项排序要求超出了元数据更新之间的排序范围。在我们可以提交为文件分配新块的事务之前，必须绝对确保该事务所创建的所有数据块实际上都已写入磁盘（我们将这些数据块称为依赖数据）。忽略这一要求实际上不会损害文件系统元数据的完整性，但可能会导致在崩溃恢复后，新文件仍然包含先前文件的内容，这既是一个一致性问题，也是一个安全风险。

> 注：元数据更新之前依赖数据盘写入。

### Merging transactions

日志型文件系统所使用的大部分术语与技术，均源自数据库领域。在该领域中，日志记录是保障复杂事务原子提交的标准机制。不过，传统数据库事务与文件系统之间存在诸多差异，而其中部分差异，能让我们对文件系统的设计进行极大简化。

其中最大的两个区别是，文件系统没有事务中止功能，而且所有文件系统事务的生命周期都相对较短。在数据库中，我们有时希望在事务进行到一半时中止它，丢弃到目前为止所做的任何更改，但在 ext2fs 中情况并非如此！当我们开始对文件系统进行任何更改时，我们已经检查过该更改可以合法完成。在开始写入更改之前中止事务（例如，如果创建文件操作发现存在同名文件，可能会中止）不会有问题，因为在这种情况下，我们只需提交没有任何更改的事务，就能达到相同的效果。

第二个差异！文件系统事务的生命周期很短！这一点很重要，因为它意味着我们可以极大地简化事务之间的依赖关系。如果我们必须处理一些非常长期的事务，那么我们需要允许事务以任何顺序独立提交，只要它们彼此不冲突，否则一个停滞的事务就可能拖慢整个系统。然而，如果所有事务都足够快，那么我们可以要求事务以严格的顺序提交到磁盘，而不会显著影响性能。

基于这一观察，我们可以对事务模型进行简化，这能显著降低实现的复杂性，同时提高性能。我们不必为每个文件系统更新创建单独的事务，而是每隔一段时间创建一个新事务，并允许所有文件系统服务调用将其更新添加到这个单一的系统级复合事务中。

这种机制有一个很大的优势。由于复合事务中的所有操作都将一起提交到日志中，我们不必为那些更新非常频繁的元数据块编写单独的副本。特别是，这对于创建新文件等操作很有帮助，在这些操作中，通常对文件的每次写入都会导致文件被扩展，从而持续更新相同的配额、位图块和索引节点块。在复合事务的生命周期内多次更新的任何块，只需提交到磁盘一次。

何时提交当前的复合事务并开始新事务，这一决定属于策略性决策，应由用户控制，因为它涉及到一种会影响系统性能的权衡。提交等待的时间越长，就有越多的文件系统操作可以合并到日志中，从长远来看所需的IO操作也就越少。然而，较长的提交会占用更多的内存和磁盘空间，并且如果发生崩溃，更新丢失的风险窗口也会更大。它们还可能导致磁盘活动激增，使得文件系统的响应时间更难预测。

### On-disk representation

带日志的 ext2fs 文件系统在磁盘上的布局将与现有的 ext2fs 内核完全兼容。传统的 UNIX 文件系统通过将每个文件与磁盘上一个唯一编号的 inode 相关联来在磁盘上存储数据，而 ext2fs 设计中已经包含了一些预留的 inode 编号。我们使用这些预留 inode 中的一个来存储文件系统日志，并且在所有其他方面，该文件系统都将与现有的 Linux 内核兼容。现有的 ext2fs 设计包含一组兼容性位图，其中的位可以被设置以表明文件系统使用了某些扩展。通过为日志扩展分配一个新的兼容性位，我们可以确保，即使旧内核能够成功挂载新的带日志的 ext2fs 文件系统，它们也不被允许以任何方式向该文件系统写入数据。

### Format of the filesystem journal

日志文件的作用很简单：在我们提交事务的过程中，它会记录文件系统元数据块的新内容。日志的另一个唯一要求是，我们必须能够原子性地提交它所包含的事务。

我们向日志中写入三种不同类型的数据块：元数据块、描述符块和头块。

日志元数据块包含由事务更新的单个文件系统元数据块的全部内容。这意味着，无论我们对文件系统元数据块所做的更改有多小，都必须写出整个日志块来记录该更改。不过，事实证明这相对成本较低，原因有二：

* 无论如何，日志写入的速度都相当可观，原因在于绝大多数日志写入操作都是顺序执行的，并且我们能够轻松地将日志 I/O 请求批量整合为大型数据块，而这类数据块可由磁盘控制器高效处理。

* 通过将已更改的元数据缓冲区的全部内容从文件系统缓存写入日志，我们避免了在日志记录代码中进行大量的CPU工作。

Linux 内核已经为我们提供了一种非常高效的机制，用于将缓冲区高速缓存中现有块的内容写入磁盘上的不同位置。缓冲区高速缓存中的每个缓冲区都由一个称为 buffer_head 的结构来描述，该结构包含有关缓冲区数据将要写入哪个磁盘块的信息。如果我们想将整个缓冲区块写入新位置而不干扰 buffer_head，只需创建一个新的临时 buffer_head，将旧 buffer_head 中的描述复制到其中，然后编辑临时 buffer_head 中的设备块数字段，使其指向日志文件内的一个块。之后，我们可以将这个临时 buffer_head 直接提交到设备 IO 系统，并在 IO 完成后将其丢弃。

描述符块是用于描述其他日志元数据块的日志块。每当我们想要将元数据块写入日志时，都需要记录这些元数据通常所在的磁盘块，以便恢复机制能够将元数据复制回主文件系统。在日志中的每组元数据块之前都会写入一个描述符块，该描述符块包含要写入的元数据块的数量及其磁盘块编号。

描述符块和元数据块都被按顺序写入日志，每当我们写到日志末尾时，就会从日志开头重新开始。我们始终维护着日志的当前头部（最后写入的块的块号）和尾部（如下所述，日志中尚未解除固定的最旧块）。每当日志空间耗尽时——即日志头部循环回绕并追上尾部时——我们就会暂停新的日志写入，直到日志尾部被清理以释放更多空间。

最后，日志文件在固定位置包含多个头部块。这些块记录了日志当前的头部和尾部，以及一个序列号。在恢复时，会扫描头部块以找到序列号最高的块，而在恢复期间扫描日志时，我们只需按照该头块中记录的那样，从尾到头遍历所有日志块。

### Committing and checkpointing the journal

在某些时候，要么是因为自上次提交后我们已经等待了足够长的时间，要么是因为日志中的空间所剩无几，我们会希望将未完成的文件系统更新作为一个新的复合事务提交到日志中。

一旦复合事务完全提交，我们的工作还未结束。我们需要跟踪事务中记录的元数据缓冲区，以便在它们被写回磁盘上的主要位置时能够察觉。

回想一下，当我们提交一个事务时，新更新的文件系统块位于日志中，但尚未同步回它们在磁盘上的永久主块（我们需要保持旧块不被同步，以防在提交日志之前崩溃）。一旦日志被提交，磁盘上的旧版本就不再重要了，我们可以从容地将缓冲区写回它们的主位置。然而，在我们完成这些缓冲区的同步之前，不能删除日志中的数据副本。

要完全提交并完成事务的检查点操作，我们会经历以下阶段：

* 1. 完成交易。此时，我们会创建一个新交易，用于记录未来开始的所有文件系统操作。任何现有的、未完成的操作仍将使用现有的交易：我们不能将单个文件系统操作拆分到多个交易中！

* 2. 开始将事务刷新到磁盘。在一个独立的日志写入器内核线程的环境中，我们开始将所有被事务修改过的元数据缓冲区写入日志。在此阶段，我们还必须写出所有相关数据（参见上文“事务的结构”部分）。当缓冲区已提交时，将其标记为锁定该事务，直到该缓冲区不再是脏的（即已通过常规回写机制写回主存储）。

* 3. 等待此事务中所有未完成的文件系统操作完成。我们可以安全地在所有操作完成之前就开始撰写日志，并且让这两个步骤在一定程度上重叠会更快。

* 4. 等待所有未完成的事务更新被完全记录到日志中。

* 5. 更新日志头块以记录日志的新头部和尾部，并将事务提交到磁盘。

* 6. 当我们将事务的更新缓冲区写入日志时，我们会将它们标记为在日志中固定该事务。只有当这些缓冲区同步到磁盘上的原始位置后，它们才会被解除固定。只有当事务的最后一个缓冲区被解除固定时，我们才能重新使用该事务所占用的日志块。当这种情况发生时，写入另一组日志头，记录日志尾部的新位置。日志中释放的空间现在可以被后续事务重新使用。

### Collisions between transactions

为了提高性能，我们在提交事务时不会完全暂停文件系统更新。相反，我们会创建一个新的复合事务，用于记录在提交旧事务时到达的更新。

这就留下了一个问题：如果某个更新想要访问一个元数据缓冲区，而该缓冲区已被另一个正在提交的较旧事务占用，该怎么办？为了提交旧事务，我们需要将其缓冲区写入日志，但在写入过程中，我们不能包含任何不属于该事务的更改，因为那样会导致我们提交不完整的更新。

如果新事务只想读取相关缓冲区，那么没有问题：我们在两个事务之间创建了一个读写依赖关系，但由于复合事务总是以严格的顺序提交，我们可以安全地忽略这种冲突。

如果新事务想要写入缓冲区，情况就会变得更加复杂。我们需要缓冲区的旧副本以提交第一个事务，但又不能不让新事务修改缓冲区就使其继续执行。

在这种情况下，解决办法是制作缓冲区的一个新副本。一个副本交给新事务进行修改，另一个则保留原有的所有权。通过旧事务，并将像往常一样提交到日志中。一旦该事务提交，这个副本就会被简单地删除。当然，在该缓冲区被安全记录到文件系统的其他位置之前，我们无法回收旧事务的日志空间，但由于缓冲区必须被提交到下一个事务的日志记录中，这一点会被自动处理。

## Project status and future work

这仍然是一项在研工作。初始实现的设计既稳定又简单，我们预计在完成实现的过程中不需要对设计进行任何重大修订。

上述设计相对简单，除了处理日志文件管理、缓冲区与事务之间的关联以及非正常关闭后文件系统恢复的代码外，对现有的 ext2fs 代码只需进行极少的修改。

一旦我们拥有一个稳定的代码库可供测试，就有许多可能的方向来扩展这一基本设计。首要任务是优化文件系统性能。这需要我们研究日志系统中各种参数的影响，例如提交频率和日志大小。同时，还需要研究系统瓶颈，以确定是否可以通过修改系统设计来提升性能，而且已经有几个可能的设计扩展方案浮现出来。

一个研究方向可能是考虑对日志更新的更新进行压缩。当前的方案要求我们将整个元数据块写入日志，即使该块中只有一个位被修改。我们可以很容易地压缩此类更新，只需在缓冲区中记录已更改的值，而不是完整记录整个缓冲区。然而，目前尚不清楚这是否会带来任何显著的性能提升。当前方案对于大多数写入操作不需要内存到内存的复制，这在 CPU 和总线利用率方面是一个很大的性能优势。写出整个缓冲区所产生的 IO 成本很低！这些更新是连续的，在现代磁盘 IO 系统上，它们直接从主内存传输到磁盘控制器，无需经过缓存或 CPU。

另一个重要的可能扩展领域是对快速 NFS 服务器的支持。NFS 设计允许客户端在服务器崩溃时平稳恢复：服务器重启时，客户端会重新连接。如果发生此类崩溃，服务器尚未安全写入磁盘的任何客户端数据都将丢失，因此 NFS 要求服务器在将客户端的文件系统请求提交到服务器磁盘之前，不得确认该请求已完成。

对于通用文件系统来说，支持这一特性可能会有些棘手。NFS 服务器的性能通常通过对客户端请求的响应时间来衡量，如果这些响应必须等待文件系统更新同步到磁盘，那么整体性能就会受到磁盘上文件系统更新延迟的限制。这与文件系统的大多数其他用途不同，在那些用途中，性能是根据缓存内更新的延迟来衡量的，而不是磁盘上的更新。

已经有一些文件系统是专门为解决这个问题而设计的。WAFL[6] 是一种基于事务树的文件系统，它可以将更新写入磁盘上的任何位置，但 Calaveras 文件系统[7] 通过使用类似于上面提出的日志来达到相同的目的。不同之处在于，Calaveras 会为每个应用程序的文件系统请求在日志中记录一个单独的事务，从而尽快完成磁盘上的单独更新。所提出的 ext2fs 日志功能中的提交批处理牺牲了这种快速提交，转而支持一次提交多个更新，以延迟为代价提高吞吐量（磁盘延迟被缓存的作用对应用程序隐藏了）。

有两种方法可以使 ext2fs 日志更适合在 NFS 服务器上使用，一是使用更小的事务，二是对文件数据和元数据都进行日志记录。通过调整提交到日志的事务大小，我们或许能够显著改善单个更新的提交周转时间。NFS 还要求数据写入尽快提交到磁盘，而且原则上我们没有理由不扩展日志文件以涵盖普通文件数据的写入。

最后，需要注意的是，该方案并不妨碍我们在多个不同的文件系统之间共享单个日志文件。实现这一点仅需少量额外工作，即可允许多个文件系统将日志记录到完全为此目的而独立保留的单独磁盘上。在多个启用日志的文件系统同时承受高负载的情况下，这可能带来显著的性能提升。该独立日志磁盘几乎完全以顺序方式写入，因此能够维持高吞吐量，同时不会影响主文件系统磁盘可用的带宽。

## Conclusions

本文概述的文件系统设计应比 Linux 上现有的 ext2fs 文件系统具有显著优势。它应通过使文件系统在崩溃后能更可预测、更快速地恢复，从而提高可用性和可靠性，并且在正常操作期间即使有性能损失，也不会太大。

对日常性能最显著的影响是，新创建的文件必须快速同步到磁盘，以便将创建操作提交到日志中，而不是像内核通常支持的那样允许数据延迟回写。这可能会使日志文件系统不适合在 /tmp 文件系统上使用。

该设计应尽可能少地修改现有的 ext2fs 代码库：大部分功能由新的日志机制提供，该机制将通过简单的事务性缓冲区 IO 接口与 ext2fs 主代码交互。

最后，本文提出的设计建立在现有的 ext2fs 磁盘文件系统布局之上，因此可以在不重新格式化文件系统的情况下，为现有的 ext2fs 文件系统添加事务日志，从而利用这些新功能。
